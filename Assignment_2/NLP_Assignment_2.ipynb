{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mjPSivFkHI09"
      },
      "outputs": [],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "from collections import Counter\n",
        "\n",
        "## Getting the top-20 frequent tags in LawSE -- There is a reason for passing 21\n",
        "def get_frequent_tags(post_parser, topk=21):\n",
        "  lst_tags = []\n",
        "  for question_id in post_parser.map_questions:\n",
        "    question = post_parser.map_questions[question_id]\n",
        "    creation_date_year = int(question.creation_date.split(\"-\")[0])\n",
        "    tag = question.tags[0]\n",
        "    lst_tags.append(tag)\n",
        "  tag_freq_dic = dict(Counter(lst_tags))\n",
        "  tag_freq_dic = dict(sorted(tag_freq_dic.items(), key=lambda item: item[1], reverse=True))\n",
        "  return list(tag_freq_dic.keys())[:topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import re\n",
        "# removes punctuation, escape characters, and html code from the text\n",
        "def preprocess_text(text):\n",
        "    #preprocess text\n",
        "    soup = BeautifulSoup(text)\n",
        "    soup_text = soup.findAll(string=True)\n",
        "    joined_soup_text = ' '.join(soup_text)\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    removed_punctuation = joined_soup_text.translate(translator)\n",
        "    final_text = re.sub(r'[\\n\\t]', ' ', removed_punctuation)\n",
        "    return final_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IUb9nbM8K9Zx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Getting dictionary of train and test samples in form of\n",
        "# key: tag value: list of tuples in form of (title, body)\n",
        "def build_train_test(post_parser, lst_frequent_tags):\n",
        "  dic_training = {}\n",
        "  dic_test = {}\n",
        "  for question_id in post_parser.map_questions:\n",
        "    question = post_parser.map_questions[question_id]\n",
        "    creation_date_year = int(question.creation_date.split(\"-\")[0])\n",
        "    tag = question.tags[0]\n",
        "    if tag in lst_frequent_tags:\n",
        "      #title = preprocess_text(question.title.lower())  \n",
        "      #body = preprocess_text(question.body.lower())\n",
        "      title = question.title.lower()\n",
        "      body = question.body.lower()\n",
        "      if creation_date_year > 2021:\n",
        "        if tag in dic_test:\n",
        "          dic_test[tag].append((title, body))\n",
        "        else:\n",
        "          dic_test[tag] = [(title, body)]\n",
        "      else:\n",
        "        if tag in dic_training:\n",
        "          dic_training[tag].append((title, body))\n",
        "        else:\n",
        "          dic_training[tag] = [(title, body)]\n",
        "  return dic_test, dic_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separates dictionaries into 3, title, body, and title+body(both)\n",
        "def build_dictionaries(dic):\n",
        "    title = {}\n",
        "    body = {}\n",
        "    both = {}\n",
        "    for key, values in dic.items():\n",
        "        for value in values:\n",
        "            title_text = preprocess_text(str(value[0]))\n",
        "            body_text = preprocess_text(str(value[1]))\n",
        "            both_text = title_text + \" \" + body_text\n",
        "            if key in title:\n",
        "                title[key].append(title_text)\n",
        "            else:\n",
        "                title[key] = [title_text]\n",
        "            if key in body:\n",
        "                body[key].append(body_text)\n",
        "            else:\n",
        "                body[key] = [body_text]\n",
        "            if key in both:\n",
        "                both[key].append(both_text)\n",
        "            else:\n",
        "                both[key] = [both_text]\n",
        "    return title, body, both\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JdQEEfJkL1YA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class\t#training\t#test\n",
            "criminal-law\t948\t78\n",
            "copyright\t2016\t181\n",
            "united-states\t5668\t863\n",
            "united-kingdom\t1195\t271\n",
            "employment\t238\t36\n",
            "international\t316\t43\n",
            "canada\t382\t35\n",
            "intellectual-property\t301\t29\n",
            "england-and-wales\t165\t138\n",
            "european-union\t219\t30\n",
            "licensing\t241\t29\n",
            "california\t391\t41\n",
            "internet\t416\t39\n",
            "business\t171\t7\n",
            "rental-property\t158\t20\n",
            "software\t292\t33\n",
            "contract-law\t1065\t111\n",
            "privacy\t351\t23\n",
            "constitutional-law\t177\t21\n",
            "gdpr\t435\t63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shady\\AppData\\Local\\Temp\\ipykernel_21224\\2736422482.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text)\n"
          ]
        }
      ],
      "source": [
        "post_parser = PostParserRecord(\"Posts_law.xml\")\n",
        "lst_frequent_tags = get_frequent_tags(post_parser)\n",
        "# We removed contract as it had no post after 2021\n",
        "lst_frequent_tags.remove(\"contract\")\n",
        "dic_test, dic_training = build_train_test(post_parser, lst_frequent_tags)\n",
        "print(\"class\\t#training\\t#test\")\n",
        "for item in dic_training:\n",
        "  print(str(item) + \"\\t\" +str(len(dic_training[item]))+\"\\t\"+str(len(dic_test[item])))\n",
        "dic_training_title, dic_training_body, dic_training_both = build_dictionaries(dic_training)\n",
        "dic_test_title, dic_test_body, dic_test_both = build_dictionaries(dic_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.priors = {}\n",
        "        self.likelihoods = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    def train(self, train_dict):\n",
        "        # Compute the prior probabilities\n",
        "        total_docs = sum(len(train_dict[tag]) for tag in train_dict)\n",
        "        for tag in train_dict:\n",
        "            self.priors[tag] = len(train_dict[tag]) / total_docs\n",
        "\n",
        "        # Set the prior probabilities to be uniform if they are all the same\n",
        "        if len(set(self.priors.values())) == 1:\n",
        "            uniform_prior = 1 / len(self.priors)\n",
        "            for tag in self.priors:\n",
        "                self.priors[tag] = uniform_prior\n",
        "\n",
        "        # Build the vocabulary\n",
        "        for tag in train_dict:\n",
        "            for text in train_dict[tag]:\n",
        "                words = text.split()\n",
        "                self.vocab.update(words)\n",
        "        # Compute the conditional probabilities with Laplace smoothing\n",
        "        alpha = 0.76  # Laplace smoothing parameter\n",
        "        for tag in train_dict:\n",
        "            tag_docs = train_dict[tag]\n",
        "            tag_word_counts = defaultdict(int)\n",
        "            for text in tag_docs:\n",
        "                words = text.split()\n",
        "                for word in words:\n",
        "                    tag_word_counts[word] += 1\n",
        "            total_words = sum(tag_word_counts.values())\n",
        "            for word in self.vocab:\n",
        "                self.likelihoods[tag][word] = (tag_word_counts[word] + alpha) / (total_words + alpha * len(self.vocab))\n",
        "    # predicts classes of text\n",
        "    def predict(self, text):\n",
        "        words = text.split()\n",
        "        probs = {tag: np.log(self.priors[tag]) for tag in self.priors}\n",
        "        for word in words:\n",
        "            if word not in self.vocab:\n",
        "                continue\n",
        "            for tag in self.likelihoods:\n",
        "                probs[tag] += np.log(self.likelihoods[tag][word])\n",
        "        return max(probs, key=probs.get)\n",
        "    # returns metrics based on the predictions\n",
        "    def evaluate(self, test_dict):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for tag in test_dict:\n",
        "            for text in test_dict[tag]:\n",
        "                y_true.append(tag)\n",
        "                y_pred.append(self.predict(text))\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "        return accuracy, precision, recall, f1_micro, f1_macro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uses previously implemented Naive Bayes Classifier to predict classes in test dictionaries\n",
        "def classify_dic(train, test):\n",
        "    classifier = NaiveBayesClassifier()\n",
        "    classifier.train(train)\n",
        "    total = 0\n",
        "    cnt = 0\n",
        "    uni = 0\n",
        "    for tag in test:\n",
        "        for text in test[tag]:\n",
        "            #print(text)\n",
        "            predicted_tag = classifier.predict(text)\n",
        "            total += 1\n",
        "            if predicted_tag == 'united-states':\n",
        "                uni += 1\n",
        "            if predicted_tag == tag:\n",
        "                #print(f\"CORRECT Predicted tag: {predicted_tag}, Actual tag: {tag}\")\n",
        "                cnt += 1\n",
        "            #else:\n",
        "                #print(f\"ERROR Predicted tag: {predicted_tag}, Actual tag: {tag}\")\n",
        "    print(\"correct predictions \" + str(cnt))\n",
        "    print(\"wrong predictions \" + str(total - cnt)) \n",
        "    print(\"total predictions \" + str(total))    \n",
        "    print(\"united states predictions \" + str(uni))\n",
        "    accuracy, precision, recall, f1_micro, f1_macro = classifier.evaluate(test)\n",
        "    print(\"accuracy \" + str(accuracy) + \" precision \" + str(precision) + \" recall \" + str(recall))\n",
        "    print(\"f1_micro \" + str(f1_micro) + \" f1_macro \" + str(f1_macro))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "title\n",
            "correct predictions 1052\n",
            "wrong predictions 1039\n",
            "total predictions 2091\n",
            "united states predictions 1505\n",
            "accuracy 0.5031085604973696 precision 0.31583656274229893 recall 0.15145062446851007\n",
            "f1_micro 0.5031085604973696 f1_macro 0.14926116449813084\n",
            "body\n",
            "correct predictions 1054\n",
            "wrong predictions 1037\n",
            "total predictions 2091\n",
            "united states predictions 1342\n",
            "accuracy 0.5040650406504065 precision 0.21828506714459972 recall 0.16495297192092373\n",
            "f1_micro 0.5040650406504065 f1_macro 0.1508106803257781\n",
            "title+body\n",
            "correct predictions 1076\n",
            "wrong predictions 1015\n",
            "total predictions 2091\n",
            "united states predictions 1268\n",
            "accuracy 0.5145863223338115 precision 0.27745889755923175 recall 0.18120504021974343\n",
            "f1_micro 0.5145863223338115 f1_macro 0.16668924695895376\n"
          ]
        }
      ],
      "source": [
        "print(\"title\")\n",
        "classify_dic(dic_training_title, dic_test_title)\n",
        "print(\"body\")\n",
        "classify_dic(dic_training_body, dic_test_body)\n",
        "print(\"title+body\")\n",
        "classify_dic(dic_training_both, dic_test_both)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import configparser\n",
        "\n",
        "# read the configuration file\n",
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "# retrieve the API key from the configuration file\n",
        "api_key = config.get('openai', 'api_key')\n",
        "\n",
        "# set the API key for the OpenAI library\n",
        "openai.api_key = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define function to ask a question and get a response from OpenAI\n",
        "def ask_openai(question):\n",
        "    # set up parameters for the OpenAI API request\n",
        "    prompt = \"Q: \" + question + \"\\nA:\"\n",
        "    model = \"text-davinci-002\"\n",
        "    temperature = 0.5\n",
        "    max_tokens = 1024\n",
        "    top_p = 1\n",
        "    frequency_penalty = 0\n",
        "    presence_penalty = 0\n",
        "\n",
        "    # make the API request and get the response\n",
        "    response = openai.Completion.create(\n",
        "        engine=model,\n",
        "        prompt=prompt,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        frequency_penalty=frequency_penalty,\n",
        "        presence_penalty=presence_penalty,\n",
        "        stop=None\n",
        "    )\n",
        "    print(\"Question asked: \" + question)\n",
        "    # extract and return the response text\n",
        "    return response.choices[0].text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question asked: can you film the police while they search your premises\n",
            "OpenAI response: It depends on the situation. If you are being detained or arrested, the police may confiscate your phone. If you are not being detained or arrested, you can film the police, but you may want to keep your distance to avoid being accused of interfering.\n",
            "Question asked: is Behrooz Mansouri a good professor?\n",
            "OpenAI response: I cannot speak for all students, but I found Professor Mansouri to be an excellent professor. He was always willing to help and was very patient. His lectures were well-organized and he made sure to cover all of the material. I would definitely recommend him to others.\n"
          ]
        }
      ],
      "source": [
        "response = ask_openai(dic_test_title[\"united-states\"][1])\n",
        "print(\"OpenAI response: \" + response)\n",
        "response = ask_openai(\"is Behrooz Mansouri a good professor?\")\n",
        "print(\"OpenAI response: \" + response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accepted answer for \"can you film the police while they search your premises\"\n",
            "Yes. The right to film police carrying out their duties is constitutionally protected. See, e.g, ACLU v. Alvarez, 679 F.3d 583 (7th Cir. 2012) and Glik v. Cunniffe, 655 F.3d 78 (1st Cir. 2011). Of course, one could always imagine some fact pattern in which some other consideration overrode this constitutional right (e.g. filming would have had the effect of destroying evidence that was light sensitive, so the police were using night vision equipment). Related: Boulder, Colorado pays $95,000 to settle the claim of a man arrested for filming the police.\n"
          ]
        }
      ],
      "source": [
        "print(\"The accepted answer for \\\"\" + dic_test_title[\"united-states\"][1] + \"\\\"\")\n",
        "print(\"Yes. The right to film police carrying out their duties is constitutionally protected. See, e.g, ACLU v. Alvarez, 679 F.3d 583 (7th Cir. 2012) and Glik v. Cunniffe, 655 F.3d 78 (1st Cir. 2011). Of course, one could always imagine some fact pattern in which some other consideration overrode this constitutional right (e.g. filming would have had the effect of destroying evidence that was light sensitive, so the police were using night vision equipment). Related: Boulder, Colorado pays $95,000 to settle the claim of a man arrested for filming the police.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "69de9970eeabaea5283f248995583f8352fbf7a76ed1f2c0988b7a9b1cdd96bb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
