{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mjPSivFkHI09"
      },
      "outputs": [],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "from collections import Counter\n",
        "\n",
        "## Getting the top-20 frequent tags in LawSE -- There is a reason for passing 21\n",
        "def get_frequent_tags(post_parser, topk=21):\n",
        "  lst_tags = []\n",
        "  for question_id in post_parser.map_questions:\n",
        "    question = post_parser.map_questions[question_id]\n",
        "    creation_date_year = int(question.creation_date.split(\"-\")[0])\n",
        "    tag = question.tags[0]\n",
        "    lst_tags.append(tag)\n",
        "  tag_freq_dic = dict(Counter(lst_tags))\n",
        "  tag_freq_dic = dict(sorted(tag_freq_dic.items(), key=lambda item: item[1], reverse=True))\n",
        "  return list(tag_freq_dic.keys())[:topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    #preprocess text\n",
        "    soup = BeautifulSoup(text)\n",
        "    soup_text = soup.findAll(string=True)\n",
        "    joined_soup_text = ' '.join(soup_text)\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    removed_punctuation = joined_soup_text.translate(translator)\n",
        "    final_text = re.sub(r'[\\n\\t]', ' ', removed_punctuation)\n",
        "    return final_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IUb9nbM8K9Zx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Getting dictionary of train and test samples in form of\n",
        "# key: tag value: list of tuples in form of (title, body)\n",
        "def build_train_test(post_parser, lst_frequent_tags):\n",
        "  dic_training = {}\n",
        "  dic_test = {}\n",
        "  for question_id in post_parser.map_questions:\n",
        "    question = post_parser.map_questions[question_id]\n",
        "    creation_date_year = int(question.creation_date.split(\"-\")[0])\n",
        "    tag = question.tags[0]\n",
        "    if tag in lst_frequent_tags:\n",
        "      #title = preprocess_text(question.title.lower())  \n",
        "      #body = preprocess_text(question.body.lower())\n",
        "      title = question.title.lower()\n",
        "      body = question.body.lower()\n",
        "      if creation_date_year > 2021:\n",
        "        if tag in dic_test:\n",
        "          dic_test[tag].append((title, body))\n",
        "        else:\n",
        "          dic_test[tag] = [(title, body)]\n",
        "      else:\n",
        "        if tag in dic_training:\n",
        "          dic_training[tag].append((title, body))\n",
        "        else:\n",
        "          dic_training[tag] = [(title, body)]\n",
        "  return dic_test, dic_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_dictionaries(dic):\n",
        "    title = {}\n",
        "    body = {}\n",
        "    both = {}\n",
        "    for key, values in dic.items():\n",
        "        for value in values:\n",
        "            title_text = preprocess_text(str(value[0]))\n",
        "            body_text = preprocess_text(str(value[1]))\n",
        "            both_text = title_text + \" \" + body_text\n",
        "            if key in title:\n",
        "                title[key].append(title_text)\n",
        "            else:\n",
        "                title[key] = [title_text]\n",
        "            if key in body:\n",
        "                body[key].append(body_text)\n",
        "            else:\n",
        "                body[key] = [body_text]\n",
        "            if key in both:\n",
        "                both[key].append(both_text)\n",
        "            else:\n",
        "                both[key] = [both_text]\n",
        "    return title, body, both\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JdQEEfJkL1YA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class\t#training\t#test\n",
            "criminal-law\t948\t78\n",
            "copyright\t2016\t181\n",
            "united-states\t5668\t863\n",
            "united-kingdom\t1195\t271\n",
            "employment\t238\t36\n",
            "international\t316\t43\n",
            "canada\t382\t35\n",
            "intellectual-property\t301\t29\n",
            "england-and-wales\t165\t138\n",
            "european-union\t219\t30\n",
            "licensing\t241\t29\n",
            "california\t391\t41\n",
            "internet\t416\t39\n",
            "business\t171\t7\n",
            "rental-property\t158\t20\n",
            "software\t292\t33\n",
            "contract-law\t1065\t111\n",
            "privacy\t351\t23\n",
            "constitutional-law\t177\t21\n",
            "gdpr\t435\t63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shady\\AppData\\Local\\Temp\\ipykernel_27736\\2736422482.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text)\n"
          ]
        }
      ],
      "source": [
        "post_parser = PostParserRecord(\"Posts_law.xml\")\n",
        "lst_frequent_tags = get_frequent_tags(post_parser)\n",
        "# We removed contract as it had no post after 2021\n",
        "lst_frequent_tags.remove(\"contract\")\n",
        "dic_test, dic_training = build_train_test(post_parser, lst_frequent_tags)\n",
        "print(\"class\\t#training\\t#test\")\n",
        "for item in dic_training:\n",
        "  print(str(item) + \"\\t\" +str(len(dic_training[item]))+\"\\t\"+str(len(dic_test[item])))\n",
        "dic_training_title, dic_training_body, dic_training_both = build_dictionaries(dic_training)\n",
        "dic_test_title, dic_test_body, dic_test_both = build_dictionaries(dic_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.priors = {}\n",
        "        self.likelihoods = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    def train(self, train_dict):\n",
        "        # Compute the prior probabilities\n",
        "        total_docs = sum(len(train_dict[tag]) for tag in train_dict)\n",
        "        for tag in train_dict:\n",
        "            self.priors[tag] = len(train_dict[tag]) / total_docs\n",
        "\n",
        "        # Set the prior probabilities to be uniform if they are all the same\n",
        "        if len(set(self.priors.values())) == 1:\n",
        "            uniform_prior = 1 / len(self.priors)\n",
        "            for tag in self.priors:\n",
        "                self.priors[tag] = uniform_prior\n",
        "\n",
        "        # Build the vocabulary\n",
        "        for tag in train_dict:\n",
        "            for text in train_dict[tag]:\n",
        "                words = text.split()\n",
        "                self.vocab.update(words)\n",
        "        # Compute the conditional probabilities with Laplace smoothing\n",
        "        alpha = 0.76  # Laplace smoothing parameter\n",
        "        for tag in train_dict:\n",
        "            tag_docs = train_dict[tag]\n",
        "            tag_word_counts = defaultdict(int)\n",
        "            for text in tag_docs:\n",
        "                words = text.split()\n",
        "                for word in words:\n",
        "                    tag_word_counts[word] += 1\n",
        "            total_words = sum(tag_word_counts.values())\n",
        "            for word in self.vocab:\n",
        "                self.likelihoods[tag][word] = (tag_word_counts[word] + alpha) / (total_words + alpha * len(self.vocab))\n",
        "\n",
        "    def predict(self, text):\n",
        "        words = text.split()\n",
        "        probs = {tag: np.log(self.priors[tag]) for tag in self.priors}\n",
        "        for word in words:\n",
        "            if word not in self.vocab:\n",
        "                continue\n",
        "            for tag in self.likelihoods:\n",
        "                probs[tag] += np.log(self.likelihoods[tag][word])\n",
        "        return max(probs, key=probs.get)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_dic(train, test):\n",
        "    classifier = NaiveBayesClassifier()\n",
        "    classifier.train(train)\n",
        "    total = 0\n",
        "    cnt = 0\n",
        "    uni = 0\n",
        "    for tag in test:\n",
        "        for text in test[tag]:\n",
        "            #print(text)\n",
        "            predicted_tag = classifier.predict(text)\n",
        "            total += 1\n",
        "            if predicted_tag == 'united-states':\n",
        "                uni += 1\n",
        "            if predicted_tag == tag:\n",
        "                #print(f\"CORRECT Predicted tag: {predicted_tag}, Actual tag: {tag}\")\n",
        "                cnt += 1\n",
        "            #else:\n",
        "                #print(f\"ERROR Predicted tag: {predicted_tag}, Actual tag: {tag}\")\n",
        "    print(\"correct predictions \" + str(cnt))\n",
        "    print(\"wrong predictions \" + str(total - cnt)) \n",
        "    print(\"total predictions \" + str(total))    \n",
        "    print(\"united states predictions \" + str(uni))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "title\n",
            "correct predictions 1052\n",
            "wrong predictions 1039\n",
            "total predictions 2091\n",
            "united states predictions 1505\n",
            "body\n",
            "correct predictions 1054\n",
            "wrong predictions 1037\n",
            "total predictions 2091\n",
            "united states predictions 1342\n",
            "title+body\n",
            "correct predictions 1076\n",
            "wrong predictions 1015\n",
            "total predictions 2091\n",
            "united states predictions 1268\n"
          ]
        }
      ],
      "source": [
        "print(\"title\")\n",
        "classify_dic(dic_training_title, dic_test_title)\n",
        "print(\"body\")\n",
        "classify_dic(dic_training_body, dic_test_body)\n",
        "print(\"title+body\")\n",
        "classify_dic(dic_training_both, dic_test_both)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "69de9970eeabaea5283f248995583f8352fbf7a76ed1f2c0988b7a9b1cdd96bb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
