{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnOA8bKfqrCQ"
   },
   "source": [
    "installing sentence BERT python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "IFaexzUikgYg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence_transformers in /home/shea.durgin/.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: tqdm in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (4.27.4)\n",
      "Requirement already satisfied: nltk in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (0.1.98)\n",
      "Requirement already satisfied: numpy in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (1.24.1)\n",
      "Requirement already satisfied: scipy in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (0.13.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/shea.durgin/.local/lib/python3.8/site-packages (from sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision->sentence_transformers) (2.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->sentence_transformers) (7.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/shea.durgin/.local/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/shea.durgin/.local/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/shea.durgin/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/shea.durgin/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.3.23)\n",
      "Requirement already satisfied: filelock in /home/shea.durgin/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/shea.durgin/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/shea.durgin/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence_transformers) (7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shea.durgin/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\n",
      "Requirement already satisfied: jinja2 in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/shea.durgin/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.6.0->sentence_transformers) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.6.0->sentence_transformers) (0.34.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/shea.durgin/.local/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/shea.durgin/.local/lib/python3.8/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: lit in /home/shea.durgin/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.6.0->sentence_transformers) (16.0.1)\n",
      "Requirement already satisfied: cmake in /home/shea.durgin/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.6.0->sentence_transformers) (3.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0duVD5wYq89Y"
   },
   "source": [
    "Reading the duplicate questions and xml file (Similar to Assignment-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "aiN_X-n2kuXh"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from post_parser_record import PostParserRecord\n",
    "\n",
    "def read_tsv_test_data(file_path):\n",
    "  # Takes in the file path for test file and generate a dictionary\n",
    "  # of question id as the key and the list of question ids similar to it\n",
    "  # as value. It also returns the list of all question ids that have\n",
    "  # at least one similar question\n",
    "  dic_similar_questions = {}\n",
    "  lst_all_test = []\n",
    "  with open(file_path) as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        question_id = int(row[0])\n",
    "        lst_similar = list(map(int, row[1:]))\n",
    "        dic_similar_questions[question_id] = lst_similar\n",
    "        lst_all_test.append(question_id)\n",
    "        lst_all_test.extend(lst_similar)\n",
    "  return dic_similar_questions, lst_all_test\n",
    "\n",
    "dic_similar_questions, lst_all_test = read_tsv_test_data(\"duplicate_questions.tsv\")\n",
    "post_reader = PostParserRecord(\"Posts_law.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4wVjhXKrEHr"
   },
   "source": [
    "Using pre-trained Quora duplicate question to encode questions and find similar questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "2g9emaNvjz6O"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edd6d4006054f868ba9b260b3c20a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# in question one, we are using the pre-trained model on quora with no further fine-tuning\n",
    "model_name = 'distilbert-base-nli-stsb-quora-ranking'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# list of text to be indexed (encoded)\n",
    "corpus = []\n",
    "# this dictionary is used as key: corpus index [0, 1, 2, ...] and value: corresponding question id\n",
    "index_to_question_id = {}\n",
    "idx = 0\n",
    "\n",
    "# indexing all the questions in the law stack exchange -- only using the question titles\n",
    "for question_id in post_reader.map_questions:\n",
    "    question = post_reader.map_questions[question_id]\n",
    "    text = question.title\n",
    "    q_id = question.post_id\n",
    "    corpus.append(text)\n",
    "    index_to_question_id[idx] = question_id\n",
    "    idx += 1\n",
    "    \n",
    "# Indexing (embedding) the \n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P@1: 0.0070921985815602835\n",
      "Average Mean Reciprocal Rank: 0.12549970266865101\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists to store P@1 and MRR values\n",
    "p_at_1_list = []\n",
    "mrr_list = []\n",
    "# Find top 100 similar questions for each question in the corpus\n",
    "for idx in index_to_question_id:\n",
    "    q_id = index_to_question_id[idx]\n",
    "    if q_id in dic_similar_questions:\n",
    "        emb = corpus_embeddings[idx]\n",
    "        # Use cosine-similarity and torch.topk to find the highest 100 scores\n",
    "        cos_scores = util.cos_sim(emb, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=100)\n",
    "\n",
    "        # Extract the indices of the top 100 matches\n",
    "        top_indices = top_results[1].tolist()\n",
    "\n",
    "\n",
    "        # Calculate P@1 and MRR for the top 100 matches\n",
    "        p_at_1 = 0\n",
    "        reciprocal_ranks = []\n",
    "        for i, index in enumerate(top_indices):\n",
    "            # If the top match is the original question, record P@1 and MRR\n",
    "            if index_to_question_id[index] in dic_similar_questions[q_id]:\n",
    "                if i == 0:\n",
    "                    p_at_1 = 1\n",
    "                reciprocal_ranks.append(1 / (i+1))\n",
    "                break\n",
    "          \n",
    "        # Append the P@1 and MRR values for this question to the lists\n",
    "        p_at_1_list.append(p_at_1)\n",
    "        if reciprocal_ranks:\n",
    "            mrr_list.append(np.mean(reciprocal_ranks))\n",
    "        else:\n",
    "            mrr_list.append(0)\n",
    "\n",
    "# Calculate the average P@1 and MRR values across all questions in the corpus\n",
    "avg_p_at_1 = np.mean(p_at_1_list)\n",
    "avg_mrr = np.mean(mrr_list)\n",
    "\n",
    "print(\"Average P@1:\", avg_p_at_1)\n",
    "print(\"Average Mean Reciprocal Rank:\", avg_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# this function uses the key for every positive sample and randomly selects a new value that isn't in the positive samples dictionary\n",
    "def generate_negative_samples(dic, lst_of_ids):\n",
    "  new_dic = {}\n",
    "  lst = []\n",
    "  cnt = 0\n",
    "  for id in dic:\n",
    "    for value in dic[id]:\n",
    "      cnt += 1\n",
    "      lst.append(value)\n",
    "    new_dic[id] = []\n",
    "    for _ in range(cnt):\n",
    "      while True:\n",
    "        random_item = random.choice(lst_of_ids)\n",
    "        if random_item not in lst:\n",
    "          break\n",
    "      new_dic[id].append(random_item)\n",
    "    cnt = 0\n",
    "  return new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper code to generate a list ids for all the questions\n",
    "lst_of_ids = []\n",
    "for id in post_reader.map_questions:\n",
    "  lst_of_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative sample id dictionaries\n",
    "negative_samples_id = generate_negative_samples(dic_similar_questions, lst_of_ids)\n",
    "positive_samples_id = dic_similar_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for training and testing, 90/10 split\n",
    "train_data = []\n",
    "test_data = {}\n",
    "test_size = int(len(positive_samples_id) * 0.1)\n",
    "cnt = 0\n",
    "for q_id in positive_samples_id:\n",
    "    if cnt < len(positive_samples_id) - test_size:\n",
    "        key_text = post_reader.map_questions[q_id].title\n",
    "        sim_q_id = positive_samples_id[q_id][0]\n",
    "        value_text = post_reader.map_questions[sim_q_id].title\n",
    "        train_data.append((key_text, value_text, 1))\n",
    "        sim_q_id = negative_samples_id[q_id][0]\n",
    "        value_text = post_reader.map_questions[sim_q_id].title\n",
    "        train_data.append((key_text, value_text, 0))\n",
    "    else:\n",
    "        test_data[q_id] = positive_samples_id[q_id][0]\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b4951808b8460dad32806e37380804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb5ac682c4e41bfabf489a39950f660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f98730211ad497492d074b46308ad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411cbec5287a4269b93e5c59964bf28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, SentencesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# using quora pre-trained model\n",
    "model_name = 'distilbert-base-nli-stsb-quora-ranking'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# build samples using both loss functions\n",
    "train_samples_MultipleNegativesRankingLoss = []\n",
    "train_samples_ConstrativeLoss = []\n",
    "\n",
    "for data in train_data:\n",
    "    train_samples_ConstrativeLoss.append(InputExample(texts=[data[0], data[1]], label=data[2]))\n",
    "    if data[2] == 1:\n",
    "        train_samples_MultipleNegativesRankingLoss.append(InputExample(texts=[data[0], data[1]], label=1))\n",
    "        train_samples_MultipleNegativesRankingLoss.append(InputExample(texts=[data[1], data[0]], label=0))  # if A is a duplicate of B, then B is a duplicate of A\n",
    "\n",
    "# Create data loader and loss for MultipleNegativesRankingLoss\n",
    "train_dataset_MultipleNegativesRankingLoss = SentencesDataset(train_samples_MultipleNegativesRankingLoss, model=model)\n",
    "train_dataloader_MultipleNegativesRankingLoss = DataLoader(train_dataset_MultipleNegativesRankingLoss, batch_size=train_batch_size)\n",
    "train_loss_MultipleNegativesRankingLoss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "distance_metric = util.pytorch_cos_sim\n",
    "margin = 0.5\n",
    "# Create data loader and loss for OnlineContrastiveLoss\n",
    "train_dataset_ConstrativeLoss = SentencesDataset(train_samples_ConstrativeLoss, model=model)\n",
    "train_dataloader_ConstrativeLoss = DataLoader(train_dataset_ConstrativeLoss, shuffle = True, batch_size=train_batch_size)\n",
    "train_loss_ConstrativeLoss = losses.OnlineContrastiveLoss(model=model, distance_metric=distance_metric, margin=margin)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader_MultipleNegativesRankingLoss, train_loss_MultipleNegativesRankingLoss), (train_dataloader_ConstrativeLoss, train_loss_ConstrativeLoss)],\n",
    "          epochs=3,\n",
    "          warmup_steps=1000,\n",
    "          output_path=model_save_path\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5e34c48fea40a6a86b1716380d9e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list of text to be indexed (encoded)\n",
    "corpus = []\n",
    "# this dictionary is used as key: corpus index [0, 1, 2, ...] and value: corresponding question id\n",
    "index_to_question_id = {}\n",
    "idx = 0\n",
    "\n",
    "# indexing all the questions in the law stack exchange -- only using the question titles\n",
    "for question_id in post_reader.map_questions:\n",
    "    question = post_reader.map_questions[question_id]\n",
    "    text = question.title\n",
    "    q_id = question.post_id\n",
    "    corpus.append(text)\n",
    "    index_to_question_id[idx] = question_id\n",
    "    idx += 1\n",
    "    \n",
    "# Indexing (embedding) the \n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P@1: 0.0\n",
      "Average Mean Reciprocal Rank: 0.1706378281750524\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists to store P@1 and MRR values\n",
    "p_at_1_list = []\n",
    "mrr_list = []\n",
    "\n",
    "# Find top 100 similar questions for each question in the corpus\n",
    "for idx in index_to_question_id:\n",
    "    q_id = index_to_question_id[idx]\n",
    "    if q_id in test_data:\n",
    "        emb = corpus_embeddings[idx]\n",
    "        \n",
    "        # Use cosine-similarity and torch.topk to find the highest 100 scores\n",
    "        cos_scores = util.cos_sim(emb, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=100)\n",
    "        \n",
    "        # Extract the indices of the top 100 matches\n",
    "        top_indices = top_results[1].tolist()\n",
    "\n",
    "        # Calculate P@1 and MRR for the top 100 matches\n",
    "        p_at_1 = 0\n",
    "        reciprocal_ranks = []\n",
    "        for i, index in enumerate(top_indices):\n",
    "            if index_to_question_id[index] in dic_similar_questions[q_id]:\n",
    "                if i == 0:\n",
    "                    p_at_1 = 1\n",
    "                reciprocal_ranks.append(1 / (i+1))\n",
    "                break\n",
    "          \n",
    "        # Append the P@1 and MRR values for this question to the lists\n",
    "        p_at_1_list.append(p_at_1)\n",
    "        if reciprocal_ranks:\n",
    "            mrr_list.append(np.mean(reciprocal_ranks))\n",
    "        else:\n",
    "            mrr_list.append(0)\n",
    "\n",
    "# Calculate the average P@1 and MRR values across all questions in the corpus\n",
    "avg_p_at_1 = np.mean(p_at_1_list)\n",
    "avg_mrr = np.mean(mrr_list)\n",
    "\n",
    "print(\"Average P@1:\", avg_p_at_1)\n",
    "print(\"Average Mean Reciprocal Rank:\", avg_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd7d8b0e88d4ab0a1499b8c49081ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quora model\n",
    "model_name = 'distilbert-base-nli-stsb-quora-ranking'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# list of text to be indexed (encoded)\n",
    "corpus = []\n",
    "# this dictionary is used as key: corpus index [0, 1, 2, ...] and value: corresponding question id\n",
    "index_to_question_id = {}\n",
    "idx = 0\n",
    "\n",
    "# indexing all the questions in the law stack exchange -- only using the question titles\n",
    "for question_id in post_reader.map_questions:\n",
    "    question = post_reader.map_questions[question_id]\n",
    "    text = question.title\n",
    "    q_id = question.post_id\n",
    "    corpus.append(text)\n",
    "    index_to_question_id[idx] = question_id\n",
    "    idx += 1\n",
    "    \n",
    "# Indexing (embedding) the \n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P@1: 0.0\n",
      "Average Mean Reciprocal Rank: 0.17041136844808843\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists to store P@1 and MRR values\n",
    "p_at_1_list = []\n",
    "mrr_list = []\n",
    "\n",
    "# Find top 100 similar questions for each question in the corpus\n",
    "for idx in index_to_question_id:\n",
    "    q_id = index_to_question_id[idx]\n",
    "    if q_id in test_data:\n",
    "        query_embedding = corpus_embeddings[idx]\n",
    "        \n",
    "        # Use cosine-similarity and torch.topk to find the highest 100 scores\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=100)\n",
    "        \n",
    "        # Extract the indices of the top 100 matches\n",
    "        top_indices = top_results[1].tolist()\n",
    "\n",
    "        # Calculate P@1 and MRR for the top 100 matches\n",
    "        p_at_1 = 0\n",
    "        reciprocal_ranks = []\n",
    "        for i, index in enumerate(top_indices):\n",
    "            if index_to_question_id[index] in dic_similar_questions[q_id]:\n",
    "                if i == 0:\n",
    "                    p_at_1 = 1\n",
    "                reciprocal_ranks.append(1 / (i+1))\n",
    "                break\n",
    "          \n",
    "        # Append the P@1 and MRR values for this question to the lists\n",
    "        p_at_1_list.append(p_at_1)\n",
    "        if reciprocal_ranks:\n",
    "            mrr_list.append(np.mean(reciprocal_ranks))\n",
    "        else:\n",
    "            mrr_list.append(0)\n",
    "\n",
    "# Calculate the average P@1 and MRR values across all questions in the corpus\n",
    "avg_p_at_1 = np.mean(p_at_1_list)\n",
    "avg_mrr = np.mean(mrr_list)\n",
    "\n",
    "print(\"Average P@1:\", avg_p_at_1)\n",
    "print(\"Average Mean Reciprocal Rank:\", avg_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra crediit legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/shea.durgin/.cache/torch/sentence_transformers/nlpaueb_legal-bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/shea.durgin/.cache/torch/sentence_transformers/nlpaueb_legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5be997d19a4b5b94bbba147b643f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use legal bert as pre-trained model\n",
    "model_name = 'nlpaueb/legal-bert-base-uncased'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# list of text to be indexed (encoded)\n",
    "corpus = []\n",
    "# this dictionary is used as key: corpus index [0, 1, 2, ...] and value: corresponding question id\n",
    "index_to_question_id = {}\n",
    "idx = 0\n",
    "\n",
    "# indexing all the questions in the law stack exchange -- only using the question titles\n",
    "for question_id in post_reader.map_questions:\n",
    "    question = post_reader.map_questions[question_id]\n",
    "    text = question.title\n",
    "    q_id = question.post_id\n",
    "    corpus.append(text)\n",
    "    index_to_question_id[idx] = question_id\n",
    "    idx += 1\n",
    "    \n",
    "# embedded corpus\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average P@1: 0.0\n",
      "Average Mean Reciprocal Rank: 0.10229937076634055\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists to store P@1 and MRR values\n",
    "p_at_1_list = []\n",
    "mrr_list = []\n",
    "\n",
    "# Find top 100 similar questions for each question in the corpus\n",
    "for idx in index_to_question_id:\n",
    "    q_id = index_to_question_id[idx]\n",
    "    if q_id in test_data:\n",
    "        query_embedding = corpus_embeddings[idx]\n",
    "        \n",
    "        # Use cosine-similarity and torch.topk to find the highest 100 scores\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=100)\n",
    "        \n",
    "        # Extract the indices of the top 100 matches\n",
    "        top_indices = top_results[1].tolist()\n",
    "\n",
    "        # Calculate P@1 and MRR for the top 100 matches\n",
    "        p_at_1 = 0\n",
    "        reciprocal_ranks = []\n",
    "        for i, index in enumerate(top_indices):\n",
    "            if index_to_question_id[index] in dic_similar_questions[q_id]:\n",
    "                if i == 0:\n",
    "                    p_at_1 = 1\n",
    "                reciprocal_ranks.append(1 / (i+1))\n",
    "                break\n",
    "          \n",
    "        # Append the P@1 and MRR values for this question to the lists\n",
    "        p_at_1_list.append(p_at_1)\n",
    "        if reciprocal_ranks:\n",
    "            mrr_list.append(np.mean(reciprocal_ranks))\n",
    "        else:\n",
    "            mrr_list.append(0)\n",
    "\n",
    "# Calculate the average P@1 and MRR values across all questions in the corpus\n",
    "avg_p_at_1 = np.mean(p_at_1_list)\n",
    "avg_mrr = np.mean(mrr_list)\n",
    "\n",
    "print(\"Average P@1:\", avg_p_at_1)\n",
    "print(\"Average Mean Reciprocal Rank:\", avg_mrr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
